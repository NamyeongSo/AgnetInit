{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd551e01",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/AgentInit/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1071\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1071\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/AgentInit/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:773\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 773\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    774\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'qwen3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 255\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m final_roles\n\u001b[0;32m--> 255\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m, in \u001b[0;36mManager.__init__\u001b[0;34m(self, llm_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/AgnetInit/experiments_Autogen/agentinit/embedder.py:12\u001b[0m, in \u001b[0;36mEmbedder.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/AgentInit/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/AgentInit/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1073\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1077\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1078\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1079\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1080\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers from source with the command \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1081\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Iterable, Type\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path so imports work when running the notebook directly\n",
    "def _find_project_root():\n",
    "    search_targets = [\n",
    "        (\"agentinit\",),\n",
    "        (\"experiments_Autogen\", \"agentinit\"),\n",
    "        (\"AgnetInit\", \"experiments_Autogen\", \"agentinit\"),\n",
    "    ]\n",
    "    for base in [Path.cwd()] + list(Path.cwd().parents):\n",
    "        for parts in search_targets:\n",
    "            candidate = base.joinpath(*parts)\n",
    "            if candidate.is_dir():\n",
    "                return candidate.parent\n",
    "    return None\n",
    "\n",
    "_project_root = _find_project_root()\n",
    "if _project_root and str(_project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(_project_root))\n",
    "_parent_root = _project_root.parent if _project_root else None\n",
    "if _parent_root and (_parent_root / \"AgentInit\").is_dir() and str(_parent_root) not in sys.path:\n",
    "    sys.path.insert(0, str(_parent_root))\n",
    "\n",
    "from agentinit.create_roles_format import CreateRoles\n",
    "# from agentinit.create_roles import CreateRoles\n",
    "from agentinit.check_roles import CheckRoles\n",
    "from agentinit.check_plans import CheckPlans\n",
    "from pydantic import BaseModel, Field\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, wait_fixed\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from agentinit.Optimizer import Optimizer\n",
    "from agentinit.select_group import SelectGroup\n",
    "from agentinit.embedder import Embedder\n",
    "import itertools\n",
    "import collections\n",
    "from vendi_score import vendi\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "class Manager(): \n",
    "    def __init__(self, llm_name):\n",
    "        self.state = 0\n",
    "        self.actions = [CreateRoles, CheckRoles, CheckPlans, SelectGroup]\n",
    "        self.todo = None\n",
    "        self.llm_name = llm_name\n",
    "        self.roles = []\n",
    "        self.role_embeddings = []\n",
    "        self.embedder = Embedder(model_path=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.groups = []\n",
    "        self.sim_matrix = None\n",
    "        self.query_sims = None\n",
    "        self.optimizer = Optimizer()\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        \"\"\"Update the current state.\"\"\"\n",
    "        self.state = state\n",
    "        self.todo = self.actions[self.state](llm_name=self.llm_name)\n",
    "\n",
    "    def Init_Population(self, min_roles=1, max_roles=None):\n",
    "        num_roles = len(self.roles)\n",
    "        groups = []\n",
    "        \n",
    "        for k in range(1, num_roles + 1):\n",
    "            groups.extend(\n",
    "                [list(indices) for indices in itertools.combinations(range(num_roles), k)]\n",
    "            )\n",
    "        \n",
    "        # print(f\"groups: {groups}\")\n",
    "        self.groups = groups\n",
    "    \n",
    "\n",
    "    async def _generate_role_embeddings(self):\n",
    "        if not self.roles:\n",
    "            return\n",
    "        prompts = [value for role in self.roles for value in role.values()]\n",
    "        \n",
    "        embeddings = self.embedder.embed_sentences(prompts)\n",
    "        \n",
    "        self.role_embeddings = embeddings.tolist() if isinstance(embeddings, np.ndarray) else embeddings\n",
    "\n",
    "\n",
    "    async def _precompute_similarities(self, query: str):\n",
    "        query_embed = self.embedder.embed_sentences([query])[0]\n",
    "        \n",
    "        embeddings = np.array(self.role_embeddings)\n",
    "        self.sim_matrix = self.embedder.cosine_similarity(embeddings)\n",
    "        self.query_sims = self.embedder.cosine_similarity_query([query_embed], embeddings)\n",
    "\n",
    "    def calculate_objective_1(self, group):\n",
    "        \"\"\"Calculate the average similarity of roles in the group with the query.\"\"\"\n",
    "        avg_sim = np.mean([self.query_sims[i] for i in group])\n",
    "        return -avg_sim\n",
    "    \n",
    "    def calculate_objective_2(self, group):\n",
    "        total_sim = 0\n",
    "        count = 0\n",
    "        submatrix = self.sim_matrix[np.ix_(group, group)]\n",
    "        score = vendi.score_K(submatrix)    \n",
    "        # print(score)\n",
    "        return -score\n",
    "\n",
    "    @retry(wait=wait_fixed(10), stop=stop_after_attempt(5))\n",
    "    async def _act(self, question):\n",
    "        roles_plan, suggestions_roles, suggestions_plan = '', '', ''\n",
    "        suggestions, num_steps = '', 2\n",
    "        steps, consensus = 0, False\n",
    "        while not consensus and steps < num_steps:\n",
    "            self._set_state(0)\n",
    "            print(\"******Create roles******\")\n",
    "            response = await self.todo.run(context = question, history=roles_plan, suggestions=suggestions)\n",
    "            if '\\n' not in response.content:\n",
    "                print(f'INVALID RESPONSE : ----{response.content}----')\n",
    "                return {'Normal':'You are a helpful assistant.'}\n",
    "            roles_plan = str(response.instruct_content)\n",
    "            if ('No Suggestions' not in suggestions_roles or 'No Suggestions' not in suggestions_plan) and steps < num_steps-1:\n",
    "            # if 'No Suggestions' not in suggestions_roles:\n",
    "                self._set_state(1)\n",
    "                history_roles = f\"## Role Suggestions\\n{suggestions_roles}\\n\\n## Feedback\\n{response.instruct_content.RoleFeedback}\"\n",
    "                print(\"******Check roles******\")\n",
    "                _suggestions_roles = await self.todo.run(response.content, history=history_roles)\n",
    "                suggestions_roles += _suggestions_roles.instruct_content.Suggestions\n",
    "                \n",
    "                self._set_state(2)\n",
    "                history_plan = f\"## Plan Suggestions\\n{suggestions_roles}\\n\\n## Feedback\\n{response.instruct_content.PlanFeedback}\"\n",
    "                print(\"******Check plans******\")\n",
    "                _suggestions_plan = await self.todo.run(response.content, history=history_plan)\n",
    "                suggestions_plan += _suggestions_plan.instruct_content.Suggestions\n",
    "\n",
    "\n",
    "            suggestions = f\"## Role Suggestions\\n{_suggestions_roles.instruct_content.Suggestions}\\n\\n## Plan Suggestions\\n{_suggestions_plan.instruct_content.Suggestions}\"\n",
    "            # suggestions = f\"## Role Suggestions\\n{_suggestions_roles.instruct_content.Suggestions}\"\n",
    "            if 'No Suggestions' in suggestions_roles and 'No Suggestions' in suggestions_plan:\n",
    "            # if 'No Suggestions' in suggestions_roles:\n",
    "                consensus = True\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        # if isinstance(response, ActionOutput):\n",
    "        #     msg = Message(content=response.content, instruct_content=response.instruct_content,\n",
    "        #                   role=self.profile, cause_by=type(self.todo))\n",
    "        # else:\n",
    "        #     msg = Message(content=response, role=self.profile, cause_by=type(self.todo))\n",
    "\n",
    "        data = str(response.instruct_content).encode().decode('unicode_escape')\n",
    "        # data = str(response.content)\n",
    "        role_matches = re.findall(r'\\{\\n\\s*\"name\":[\\s\\S]*?\\s*\\n\\}', data, re.DOTALL)\n",
    "        # print(role_matches)\n",
    "        steps_match = re.search(r'Execution Plan=(.*?)RoleFeedback', data, re.DOTALL)\n",
    "        # steps_match = re.search(r'Execution Plan:\\s*(.*?)\\s*##RoleFeedback', data, re.DOTALL)\n",
    "        # print(steps_match)\n",
    "        # print(\"steps_match:\",steps_match)\n",
    "        \n",
    "        roles = {}\n",
    "        for match in role_matches:\n",
    "            # print(\"match:\",match)\n",
    "            # match = match.replace(r'\\n','')\n",
    "            # match = match.replace(r'\\n','').replace('\\\\',r'\\\\')\n",
    "            # print(match)\n",
    "            role_data = json.loads(match.replace(\"\\\\\",r\"\\\\\"))\n",
    "            roles[role_data[\"name\"]] = role_data[\"prompt\"]\n",
    "\n",
    "        steps = []\n",
    "        seen_steps = set()  \n",
    "        # print(roles)\n",
    "        if steps_match:\n",
    "            steps_text = steps_match.group(1)+\"\\n\"\n",
    "            # print(steps_text)\n",
    "            step_matches = re.findall(r\"\\d+\\.\\s*\\**\\[(.*?)\\]\\**\\s*:(\\s.*?)\\n\", steps_text, re.DOTALL)\n",
    "            # print(step_matches)\n",
    "            for role, action in step_matches:\n",
    "                step_tuple = (role, action) \n",
    "                for role_key in roles.keys():\n",
    "                    if role_key not in seen_steps and role_key in role:\n",
    "                        seen_steps.add(role_key)\n",
    "                        steps.append({\"role\": role_key, \"action\": action})\n",
    "        # print(\"roles:\",roles)\n",
    "        # print(\"steps_match\",steps_match.group(1))\n",
    "        # print(\"steps:\",steps)\n",
    "        roles_in_steps={}\n",
    "        for step in steps:    \n",
    "            role = step[\"role\"]\n",
    "            roles_in_steps[role] = roles[role]\n",
    "        # print(f\"question = {question}\")\n",
    "        # print(f\"response = {response.instruct_content}\")\n",
    "        # print(\"roles_in_steps:\",roles_in_steps)\n",
    "\n",
    "        if not roles_in_steps:\n",
    "            raise\n",
    "\n",
    "        for role in roles_in_steps.keys():\n",
    "            role_dict = {role: roles_in_steps[role]}\n",
    "            self.roles.append(role_dict)\n",
    "        \n",
    "\n",
    "        \n",
    "        await self._generate_role_embeddings()\n",
    "        # print(\"------EMBEDDINGS------\")\n",
    "        # for role_dict, embedding in zip(self.roles, self.role_embeddings):\n",
    "            # role_name = list(role_dict.keys())[0]\n",
    "            # print(f\"{role_name}: {embedding[:5]}...\")\n",
    "\n",
    "        await self._precompute_similarities(question)\n",
    "\n",
    "        role_names = [list(role.keys())[0] for role in self.roles]\n",
    "\n",
    "        print(\"------ SIMILARITY MATRIX ------\")\n",
    "        print(\"        \" + \"  \".join(f\"{name[:6]:<6}\" for name in role_names))\n",
    "        for i, (name, row) in enumerate(zip(role_names, self.sim_matrix)):\n",
    "            print(f\"{name[:6]:<6} \" + \"  \".join(f\"{val:.3f}\" for val in row)) \n",
    "\n",
    "        print(\"\\n------ QUERY SIMILARITIES ------\")\n",
    "        for name, sim in zip(role_names, self.query_sims):\n",
    "            print(f\"Query â†’ {name}: {sim:.3f}\")\n",
    "\n",
    "        self.Init_Population()\n",
    "\n",
    "        # Step2. \n",
    "        objectives = []\n",
    "        for group in self.groups:\n",
    "            objective1 = self.calculate_objective_1(group)\n",
    "            objective2 = self.calculate_objective_2(group)\n",
    "            objectives.append((objective1, objective2))\n",
    "        # print(\"Objectives:\", objectives)\n",
    "        front = self.optimizer.fast_non_dominated_sort(objectives)\n",
    "        text = \"\"\n",
    "\n",
    "        for i,group_idx in enumerate(front[0]):\n",
    "            text += f\"Group {i+1}:\\n\"\n",
    "            group = self.groups[group_idx]\n",
    "            for idx in group:\n",
    "                text += f\"Role: {list(self.roles[idx].keys())[0]}, Prompt: {list(self.roles[idx].values())[0]}\\n\"\n",
    "        # Step3. SelectGroup\n",
    "        self._set_state(3)\n",
    "        choice_num = await self.todo.run(context=question, groups=text)\n",
    "        # print(\"------ SELECTED GROUP ------\")\n",
    "        group_idx = front[0][choice_num-1]\n",
    "        if choice_num != len(front[0]):\n",
    "            print(f\"***************\\nSelected group: {choice_num}\\nGroups:{text}\\n****************\\n\")\n",
    "        final_roles = collections.defaultdict(str)\n",
    "        for role_idx in self.groups[group_idx]:\n",
    "            final_roles[list(self.roles[role_idx].keys())[0]] = list(self.roles[role_idx].values())[0]\n",
    "\n",
    "        # print(\"------ FINAL ROLES ------\")\n",
    "        # print(final_roles)\n",
    "        \n",
    "        if not final_roles:\n",
    "            raise\n",
    "        return final_roles\n",
    "    \n",
    "\n",
    "manager = Manager(llm_name =\"Qwen/Qwen3-8B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgentInit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
